{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb994cb",
   "metadata": {},
   "source": [
    "\n",
    "# House Price Prediction — Complete ML Pipeline (Module 6)\n",
    "\n",
    "This notebook performs the full workflow on the **Ames Housing** dataset (Kaggle: 1460 rows, ~80 features):\n",
    "\n",
    "1. **EDA**  \n",
    "2. **Simple Linear Regression** (SalePrice ~ Area/GrLivArea)  \n",
    "3. **Multiple Linear Regression**  \n",
    "4. **Dimensionality Reduction** (PCA-equivalent via TruncatedSVD for sparse one-hot features) + Linear Regression  \n",
    "5. **Lasso & Ridge Regression**  \n",
    "6. **Support Vector Regression (SVR)**  \n",
    "7. **Decision Tree Regressor**  \n",
    "8. **Random Forest Regressor**  \n",
    "9. **Hyperparameter Tuning** with `GridSearchCV` and `RandomizedSearchCV`  \n",
    "10. **Model Selection**: compare all models on a common test set (R², RMSE, MAE) and save each model with a different file name.\n",
    "\n",
    "> **Instructions**  \n",
    "> - Put your dataset file (`train.csv`) from Kaggle in the same folder as this notebook.  \n",
    "> - If your dataset uses a different file name or has a column named `Area`, update the config cell accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Config ======\n",
    "DATA_PATH = \"train.csv\"  # change if needed\n",
    "TARGET_COL = \"SalePrice\"\n",
    "AREA_COL_CANDIDATES = [\"GrLivArea\", \"Area\"]  # tries first that exists\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_JOBS = -1  # use all cores\n",
    "\n",
    "# Output paths\n",
    "MODELS_DIR = \"models\"\n",
    "RESULTS_PATH = \"model_results.csv\"\n",
    "\n",
    "import os\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(\"Models will be saved to:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d97dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Imports ======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e398e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Load Data ======\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe8251",
   "metadata": {},
   "source": [
    "## 1) EDA — Structure & Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nMissing values (top 30):\")\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "display(missing.head(30))\n",
    "\n",
    "# Basic target stats\n",
    "print(\"\\nTarget stats:\")\n",
    "display(df[TARGET_COL].describe(percentiles=[.05,.25,.5,.75,.95]))\n",
    "\n",
    "# Target distribution plot (matplotlib only)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[TARGET_COL].dropna(), bins=50)\n",
    "plt.title(\"SalePrice Distribution\")\n",
    "plt.xlabel(\"SalePrice\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad448957",
   "metadata": {},
   "source": [
    "### Numeric correlations with SalePrice (Top 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(TARGET_COL)\n",
    "corr = df[numeric_cols.tolist() + [TARGET_COL]].corr(numeric_only=True)[TARGET_COL].drop(TARGET_COL).sort_values(ascending=False)\n",
    "top15 = corr.head(15)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(top15.index, top15.values)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Top 15 numeric correlations with SalePrice\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Feature columns ======\n",
    "# Determine area column for simple regression\n",
    "AREA_COL = None\n",
    "for c in AREA_COL_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        AREA_COL = c\n",
    "        break\n",
    "\n",
    "if AREA_COL is None:\n",
    "    raise ValueError(\"No area column found. Make sure 'GrLivArea' or 'Area' exists in the dataset.\")\n",
    "\n",
    "print(\"Using area column for Simple Linear Regression:\", AREA_COL)\n",
    "\n",
    "# Split train/test once for fair comparison\n",
    "X_full = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Identify numeric & categorical columns\n",
    "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "len(num_features), len(cat_features), AREA_COL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Helpers ======\n",
    "def evaluate_and_save(model, name, X_test, y_test, save_dir=MODELS_DIR):\n",
    "    preds = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    path = os.path.join(save_dir, f\"{name}.pkl\")\n",
    "    import joblib\n",
    "    joblib.dump(model, path)\n",
    "    return {\"model\": name, \"r2\": r2, \"rmse\": rmse, \"mae\": mae, \"path\": path}\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274c606",
   "metadata": {},
   "source": [
    "## 2) Simple Linear Regression (SalePrice ~ Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd752714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline: impute numeric, select AREA_COL, fit LinearRegression\n",
    "X_train_area = X_train[[AREA_COL]].copy()\n",
    "X_test_area = X_test[[AREA_COL]].copy()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "area_imputer = SimpleImputer(strategy='median')\n",
    "X_train_area_imputed = area_imputer.fit_transform(X_train_area)\n",
    "X_test_area_imputed = area_imputer.transform(X_test_area)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_simple = LinearRegression()\n",
    "lin_simple.fit(X_train_area_imputed, y_train)\n",
    "\n",
    "res = evaluate_and_save(lin_simple, \"model_simple_linear_area\", X_test_area_imputed, y_test)\n",
    "results.append(res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec093fd",
   "metadata": {},
   "source": [
    "## 3) Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8564429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe_lin = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_lin.fit(X_train, y_train)\n",
    "res = evaluate_and_save(pipe_lin, \"model_linear_multiple\", X_test, y_test)\n",
    "results.append(res)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f063dee",
   "metadata": {},
   "source": [
    "## 4) Dimensionality Reduction (TruncatedSVD as PCA for OHE) + Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b96da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_svd_lin = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"svd\", TruncatedSVD(n_iter=10, random_state=RANDOM_STATE)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid_svd = {\n",
    "    \"svd__n_components\": [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "grid_svd = GridSearchCV(pipe_svd_lin, param_grid=param_grid_svd, cv=5, n_jobs=-1)\n",
    "grid_svd.fit(X_train, y_train)\n",
    "\n",
    "best_svd_model = grid_svd.best_estimator_\n",
    "res = evaluate_and_save(best_svd_model, \"model_linear_with_svd\", X_test, y_test)\n",
    "results.append(res)\n",
    "\n",
    "grid_svd.best_params_, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da87fe",
   "metadata": {},
   "source": [
    "## 5) Lasso & Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ridge\n",
    "pipe_ridge = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", Ridge(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid_ridge = {\n",
    "    \"model__alpha\": [0.1, 1.0, 3.0, 10.0, 30.0, 100.0]\n",
    "}\n",
    "\n",
    "grid_ridge = GridSearchCV(pipe_ridge, param_grid=param_grid_ridge, cv=5, n_jobs=-1)\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "\n",
    "res = evaluate_and_save(grid_ridge.best_estimator_, \"model_ridge\", X_test, y_test)\n",
    "results.append(res)\n",
    "\n",
    "# Lasso\n",
    "pipe_lasso = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", Lasso(random_state=RANDOM_STATE, max_iter=20000))\n",
    "])\n",
    "\n",
    "param_grid_lasso = {\n",
    "    \"model__alpha\": [0.0005, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
    "}\n",
    "\n",
    "grid_lasso = GridSearchCV(pipe_lasso, param_grid=param_grid_lasso, cv=5, n_jobs=-1)\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "\n",
    "res2 = evaluate_and_save(grid_lasso.best_estimator_, \"model_lasso\", X_test, y_test)\n",
    "results.append(res2)\n",
    "\n",
    "grid_ridge.best_params_, grid_lasso.best_params_, res, res2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4fcb4",
   "metadata": {},
   "source": [
    "## 6) Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe_svr = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", SVR())\n",
    "])\n",
    "\n",
    "param_dist_svr = {\n",
    "    \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"model__C\": [0.1, 1, 10, 30, 100],\n",
    "    \"model__epsilon\": [0.01, 0.1, 0.2, 0.3],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "rand_svr = RandomizedSearchCV(\n",
    "    pipe_svr, param_distributions=param_dist_svr, n_iter=20,\n",
    "    cv=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "rand_svr.fit(X_train, y_train)\n",
    "\n",
    "best_svr = rand_svr.best_estimator_\n",
    "res = evaluate_and_save(best_svr, \"model_svr\", X_test, y_test)\n",
    "results.append(res)\n",
    "\n",
    "rand_svr.best_params_, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cdb24",
   "metadata": {},
   "source": [
    "## 7) Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eebe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_dt = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    \"model__max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(pipe_dt, param_grid=param_grid_dt, cv=5, n_jobs=-1)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "best_dt = grid_dt.best_estimator_\n",
    "res = evaluate_and_save(best_dt, \"model_decision_tree\", X_test, y_test)\n",
    "results.append(res)\n",
    "\n",
    "grid_dt.best_params_, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d811e0a",
   "metadata": {},
   "source": [
    "## 8) Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_dense),\n",
    "    (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_rf = {\n",
    "    \"model__n_estimators\": [100, 200, 300, 500, 800],\n",
    "    \"model__max_depth\": [None, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"auto\", \"sqrt\", 0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "rand_rf = RandomizedSearchCV(\n",
    "    pipe_rf, param_distributions=param_dist_rf, n_iter=25,\n",
    "    cv=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "rand_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf = rand_rf.best_estimator_\n",
    "res = evaluate_and_save(best_rf, \"model_random_forest\", X_test, y_test)\n",
    "results.append(res)\n",
    "\n",
    "rand_rf.best_params_, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e202f1b",
   "metadata": {},
   "source": [
    "## 9–10) Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"rmse\")\n",
    "results_df.to_csv(\"model_results.csv\", index=False)\n",
    "display(results_df)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(results_df[\"model\"], results_df[\"rmse\"])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Model RMSE (lower is better)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Results saved to: model_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259cbdb5",
   "metadata": {},
   "source": [
    "\n",
    "> **Note on LDA:** Linear Discriminant Analysis (LDA) is primarily a **classification** technique and not suitable for a continuous target like `SalePrice`.  \n",
    "> Therefore, PCA-equivalent dimensionality reduction via **TruncatedSVD** is used for the regression setting here.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
