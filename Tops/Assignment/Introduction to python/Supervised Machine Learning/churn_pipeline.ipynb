{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738646d6",
   "metadata": {},
   "source": [
    "\n",
    "# Customer Churn Prediction — End-to-End ML Pipeline (Module 6 — Task 2)\n",
    "\n",
    "This notebook targets the Kaggle Credit Card Customer Attrition/Churn dataset and similar churn datasets.\n",
    "It includes:\n",
    "\n",
    "1. **EDA**\n",
    "2. **Imbalance check** and resolution (simple random **over-sampling** on train only)\n",
    "3. **Logistic Regression**\n",
    "4. **Naive Bayes**\n",
    "5. **K-Nearest Neighbors**\n",
    "6. **SVC** with **GridSearchCV**\n",
    "7. **Decision Tree** with **GridSearchCV**\n",
    "8. **Random Forest** with **RandomizedSearchCV**\n",
    "9. **Model Selection**: accuracy, precision, recall, F1, ROC-AUC, confusion matrix, and saves every model separately.\n",
    "\n",
    "> **Instructions**\n",
    "> - Place your churn CSV (e.g., `BankChurners.csv`) in the **same folder** as this notebook.\n",
    "> - Update `DATA_PATH` and `TARGET_CANDIDATES` if your column names differ.\n",
    "> - The notebook auto-detects the target label among common names like `Attrition_Flag`, `Churn`, `Exited`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Config ======\n",
    "DATA_PATH = \"BankChurners.csv\"  # change if needed\n",
    "TARGET_CANDIDATES = [\"Attrition_Flag\", \"Churn\", \"Exited\", \"churn\", \"attrition_flag\"]\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_JOBS = -1  # use all cores\n",
    "\n",
    "# Output\n",
    "MODELS_DIR = \"models_churn\"\n",
    "RESULTS_PATH = \"churn_model_results.csv\"\n",
    "\n",
    "import os\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(\"Models will be saved to:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Imports ======\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, RocCurveDisplay)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Load Data ======\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec930876",
   "metadata": {},
   "source": [
    "## Target detection & label cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to detect the churn/attrition column\n",
    "target_col = None\n",
    "for c in TARGET_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        target_col = c\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    raise ValueError(f\"Could not find target column in {TARGET_CANDIDATES}. Please update TARGET_CANDIDATES or rename your column.\")\n",
    "\n",
    "print(\"Detected target column:\", target_col)\n",
    "\n",
    "# Normalize binary labels to 0/1\n",
    "y_raw = df[target_col].astype(str).str.strip()\n",
    "\n",
    "# Common mappings\n",
    "mapping_yes_no = {\"yes\":1,\"no\":0,\"true\":1,\"false\":0,\"1\":1,\"0\":0}\n",
    "mapping_attrition = {\"Attrited Customer\":1, \"Existing Customer\":0,\n",
    "                     \"Churned\":1, \"Not Churned\":0,\n",
    "                     \"Exited\":1, \"Stayed\":0}\n",
    "\n",
    "y = y_raw.str.lower().map(mapping_yes_no)\n",
    "if y.isna().any():\n",
    "    # Try attrition mapping (case-sensitive original)\n",
    "    y2 = y_raw.map(mapping_attrition)\n",
    "    y = np.where(pd.isna(y), y2, y).astype(float)\n",
    "\n",
    "# If still NaN, try a fallback binary encoding of the most frequent values\n",
    "if pd.isna(y).mean() > 0:\n",
    "    top_vals = y_raw.value_counts().index.tolist()\n",
    "    if len(top_vals) >= 2:\n",
    "        primary, secondary = top_vals[0], top_vals[1]\n",
    "        y = y_raw.apply(lambda v: 1 if v == primary else (0 if v == secondary else np.nan))\n",
    "\n",
    "# Final check\n",
    "if pd.isna(y).any():\n",
    "    raise ValueError(\"Unable to normalize target to 0/1. Please adjust mappings.\")\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "# Define X\n",
    "X = df.drop(columns=[target_col])\n",
    "print(\"Class balance (raw):\")\n",
    "print(pd.Series(y).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36f426",
   "metadata": {},
   "source": [
    "## 1) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns:\", X.columns.tolist())\n",
    "print(\"\\nDtypes summary:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nMissing values (top 30):\")\n",
    "missing = X.isna().sum().sort_values(ascending=False)\n",
    "display(missing.head(30))\n",
    "\n",
    "# Basic numeric EDA\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(num_cols) > 0:\n",
    "    desc = X[num_cols].describe(percentiles=[.05,.25,.5,.75,.95])\n",
    "    display(desc)\n",
    "\n",
    "# Churn rate\n",
    "churn_rate = y.mean()\n",
    "print(f\"Churn rate: {churn_rate:.3f}\")\n",
    "\n",
    "# Plot churn distribution\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar([\"Not Churned (0)\", \"Churned (1)\"], [ (y==0).sum(), (y==1).sum() ])\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eddbb1",
   "metadata": {},
   "source": [
    "## 2) Imbalance check & resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train distribution:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "def simple_random_oversample(Xt, yt, random_state=RANDOM_STATE):\n",
    "    # Oversample minority class to match majority count (train only)\n",
    "    counts = pd.Series(yt).value_counts()\n",
    "    if len(counts) != 2:\n",
    "        return Xt, yt\n",
    "    maj_class = counts.idxmax()\n",
    "    min_class = counts.idxmin()\n",
    "    n_maj = counts.max()\n",
    "    n_min = counts.min()\n",
    "    if n_min == 0 or n_min == n_maj:\n",
    "        return Xt, yt\n",
    "    # Split by class\n",
    "    Xm = Xt[yt==min_class]\n",
    "    ym = yt[yt==min_class]\n",
    "    reps = n_maj - n_min\n",
    "    Xm_up = Xm.sample(reps, replace=True, random_state=random_state)\n",
    "    ym_up = ym.sample(reps, replace=True, random_state=random_state)\n",
    "    X_bal = pd.concat([Xt, Xm_up], axis=0)\n",
    "    y_bal = pd.concat([yt, ym_up], axis=0)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "X_train_bal, y_train_bal = simple_random_oversample(X_train.copy(), y_train.copy())\n",
    "\n",
    "print(\"After over-sampling (train):\")\n",
    "print(pd.Series(y_train_bal).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596c625",
   "metadata": {},
   "source": [
    "## Preprocessing (impute, scale, one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features = X_train_bal.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X_train_bal.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_save(model, name, X_test, y_test, save_dir=MODELS_DIR):\n",
    "    preds = model.predict(X_test)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probas = model.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        # decision_function for SVC; fallback to scaled 0-1 via rank if not available\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            s = model.decision_function(X_test)\n",
    "            # min-max scale\n",
    "            probas = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "        else:\n",
    "            probas = preds  # not ideal, but keeps pipeline consistent\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds, zero_division=0)\n",
    "    rec = recall_score(y_test, preds, zero_division=0)\n",
    "    f1 = f1_score(y_test, preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, probas)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    path = os.path.join(save_dir, f\"{name}.pkl\")\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(f\"Confusion Matrix — {name}:\\n\", cm)\n",
    "\n",
    "    return {\"model\": name, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc, \"path\": path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3985462",
   "metadata": {},
   "source": [
    "## 3) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=N_JOBS))\n",
    "])\n",
    "pipe_lr.fit(X_train_bal, y_train_bal)\n",
    "res_lr = evaluate_and_save(pipe_lr, \"churn_logistic_regression\", X_test, y_test)\n",
    "res_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddb9d7",
   "metadata": {},
   "source": [
    "## 4) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use GaussianNB over the transformed numeric space; for categorical-heavy datasets, CategoricalNB would require integer-encoded categories.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build a pipeline that outputs dense arrays for NB\n",
    "numeric_transformer_nb = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer_nb = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "])\n",
    "preprocessor_nb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_nb, num_features),\n",
    "        (\"cat\", categorical_transformer_nb, cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor_nb),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "pipe_nb.fit(X_train_bal, y_train_bal)\n",
    "res_nb = evaluate_and_save(pipe_nb, \"churn_naive_bayes\", X_test, y_test)\n",
    "res_nb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe4071",
   "metadata": {},
   "source": [
    "## 5) K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid_knn = {\n",
    "    \"model__n_neighbors\": [3,5,7, nine := 9, 11, 15],\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "    \"model__p\": [1,2]\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid=param_grid_knn, cv=5, n_jobs=N_JOBS)\n",
    "grid_knn.fit(X_train_bal, y_train_bal)\n",
    "best_knn = grid_knn.best_estimator_\n",
    "res_knn = evaluate_and_save(best_knn, \"churn_knn\", X_test, y_test)\n",
    "grid_knn.best_params_, res_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985d96f",
   "metadata": {},
   "source": [
    "## 6) SVC with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_svc = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVC(class_weight=\"balanced\", probability=True))\n",
    "])\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"model__C\": [0.1, 1, 10, 30, 100],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(pipe_svc, param_grid=param_grid_svc, cv=5, n_jobs=N_JOBS)\n",
    "grid_svc.fit(X_train_bal, y_train_bal)\n",
    "best_svc = grid_svc.best_estimator_\n",
    "res_svc = evaluate_and_save(best_svc, \"churn_svc\", X_test, y_test)\n",
    "grid_svc.best_params_, res_svc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b62c4f",
   "metadata": {},
   "source": [
    "## 7) Decision Tree with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d60c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_dt = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    \"model__max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(pipe_dt, param_grid=param_grid_dt, cv=5, n_jobs=N_JOBS)\n",
    "grid_dt.fit(X_train_bal, y_train_bal)\n",
    "best_dt = grid_dt.best_estimator_\n",
    "res_dt = evaluate_and_save(best_dt, \"churn_decision_tree\", X_test, y_test)\n",
    "grid_dt.best_params_, res_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9efb3",
   "metadata": {},
   "source": [
    "## 8) Random Forest with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9404bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_dist_rf = {\n",
    "    \"model__n_estimators\": [100, 200, 300, 500, 800],\n",
    "    \"model__max_depth\": [None, 10, 20, 30, 50],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", 0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "rand_rf = RandomizedSearchCV(\n",
    "    pipe_rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "rand_rf.fit(X_train_bal, y_train_bal)\n",
    "best_rf = rand_rf.best_estimator_\n",
    "res_rf = evaluate_and_save(best_rf, \"churn_random_forest\", X_test, y_test)\n",
    "rand_rf.best_params_, res_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9474a",
   "metadata": {},
   "source": [
    "## 9) Model Selection & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708440e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = [res_lr, res_nb, res_knn, res_svc, res_dt, res_rf]\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
    "results_df.to_csv(RESULTS_PATH, index=False)\n",
    "display(results_df)\n",
    "\n",
    "# Plot F1\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.bar(results_df[\"model\"], results_df[\"f1\"])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Model F1 Score (higher is better)\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Results saved to:\", RESULTS_PATH)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
